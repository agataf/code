{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level RNN model\n",
    "\n",
    "Based on Python version by Alex Beatson\n",
    "\n",
    "Translation to Julia by Sebastian Seung\n",
    "\n",
    "Data I/O adapted from Andrej Karpathy's CharRNN gist: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "See his blog post for some fun applications of RNNs: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "BSD License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design notes:\n",
    "The composite type `RNN` contains the parameters and hyperparameters and nodes of the computation graph. \n",
    "\n",
    "Methods are associated with types through [multiple dispatch](https://docs.julialang.org/en/stable/manual/methods/#man-methods).\n",
    "\n",
    "Methods preceded by underscore (e.g. `_init_params`, `_rnn_step`) contain TF functions and are used to build the computation graphs for training and sampling. Placeholders are defined in `_build_graph`.\n",
    "\n",
    "Methods without underscore (`run_train`, `run_sample`) run a TF session and feed placeholder values but otherwise contain no TF functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student note:\n",
    "You should focus on understanding the RNN methods `_init_params`, `_rnn_step`, and `_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using Distributions   # for sampling from Categorical distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RNN` is a [composite type](https://docs.julialang.org/en/stable/manual/types/#composite-types) in Julia, which is analogous to an object in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_build_graph (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type RNN\n",
    "    \"\"\" hyperparameters (initialized by inner constructor)\"\"\"\n",
    "    batch_size\n",
    "    embedding_size\n",
    "    hidden_size\n",
    "    vocab_size      # number of chars in vocab\n",
    "    seq_length      # number of time steps to unroll the RNN\n",
    "    initial_learning_rate\n",
    "    decay_steps     # not used, as annealing the learning rate is not currently implemented in Julia version\n",
    "    decay_factor    #\n",
    "\n",
    "    global_step\n",
    "\n",
    "    \"\"\" neural net weights and biases \"\"\"\n",
    "    embedding\n",
    "    U\n",
    "    W\n",
    "    bh\n",
    "    V\n",
    "    by\n",
    "\n",
    "    \"\"\"Sampling graph\"\"\"\n",
    "    sample_input_char\n",
    "    sample_input_hidden\n",
    "    next_y\n",
    "    next_hidden\n",
    "\n",
    "    \"\"\"Training graph\"\"\"\n",
    "    inputs\n",
    "    targets\n",
    "    predictions\n",
    "    cost\n",
    "    train_step\n",
    "    learning_rate\n",
    "\n",
    "    sess\n",
    "\n",
    "    # inner constructor\n",
    "    RNN(batch_size, embedding_size, hidden_size, vocab_size, seq_length, initial_learning_rate, decay_steps, decay_factor, global_step = Variable(0.0, trainable=false) ) =\n",
    "        new(batch_size, embedding_size, hidden_size, vocab_size, seq_length, initial_learning_rate, decay_steps, decay_factor, global_step)\n",
    "end\n",
    "\n",
    "function _init_params(self::RNN)    # Create/initialize the trainable parameters\n",
    "    self.embedding = Variable(0.2*randn(Float32, self.vocab_size, self.embedding_size))   # learn embedding for each character\n",
    "    self.U = Variable(0.2*randn(Float32, self.embedding_size, self.hidden_size))\n",
    "    self.W = Variable(0.2*randn(Float32, self.hidden_size, self.hidden_size))\n",
    "    self.bh = Variable(zeros(Float32, 1, self.hidden_size))\n",
    "    self.V = Variable(0.2*randn(Float32, self.hidden_size, self.vocab_size))\n",
    "    self.by = Variable(zeros(Float32, 1, self.vocab_size))\n",
    "end\n",
    "\n",
    "function _rnn_step(self::RNN, x, h)\n",
    "    \"\"\"Performs RNN computation for one timestep:\n",
    "        takes a previous x and h, and computes y (prediction of next x) and h.\n",
    "            \n",
    "        In practical applications, you should almost always use TensorFlow's built-in RNN cells,\n",
    "        from tf.contrib.rnn. However for teaching purposes we are writing the RNN from scratch here.\n",
    "    \"\"\"\n",
    "    h = nn.sigmoid(x * self.U + h * self.W + self.bh)\n",
    "    y = h * self.V + self.by\n",
    "\n",
    "    return y, h\n",
    "end\n",
    "    \n",
    "function _forward(self::RNN, inputs)\n",
    "    \"\"\"Performs the forward pass for all timesteps in a sequence.\"\"\"\n",
    "\n",
    "    # Create list to hold y\n",
    "    y = Array{TensorFlow.Tensor{Float32},1}(self.seq_length)\n",
    "    \n",
    "    # Create zero-d initial hidden state\n",
    "    h = constant(zeros(Float32, self.batch_size, self.hidden_size))\n",
    "    \n",
    "    for t = 1:self.seq_length\n",
    "        x = cast(nn.embedding_lookup(self.embedding, inputs[:, t]),Float32)\n",
    "        y[t], h = _rnn_step(self, x, h)\n",
    "    end\n",
    "\n",
    "    return y\n",
    "end\n",
    "    \n",
    "function _rnn_step_char(self::RNN, input_character, h)\n",
    "    \"\"\"This is like _rnn_step, except that the input is a character rather than an embedding vector.\n",
    "\n",
    "       This is used for sequence generation w/o having to alter\n",
    "       the tensorflow graph.\"\"\"\n",
    "\n",
    "    # We expand dims because tf expects a batch\n",
    "    character = expand_dims(input_character, 1)\n",
    "\n",
    "    # Get the embedding for the input character\n",
    "    x = nn.embedding_lookup(self.embedding, character)\n",
    "    \n",
    "    # Perform the RNN look up\n",
    "    y, h = _rnn_step(self, x, h)\n",
    "\n",
    "    return y, h\n",
    "end\n",
    "\n",
    "function _build_graph(self::RNN)\n",
    "    \"\"\"Build the computation graphs for training and sampling.\n",
    "\n",
    "        All placeholders are defined in this method.\"\"\"\n",
    "\n",
    "    \"\"\"Sampling graph\"\"\"\n",
    "    self.sample_input_char = placeholder(Int32, shape=[])\n",
    "    self.sample_input_hidden = placeholder(Float32, shape=[1, self.hidden_size])\n",
    "    \n",
    "    self.next_y, self.next_hidden = _rnn_step_char(self,\n",
    "       self.sample_input_char, self.sample_input_hidden)\n",
    "\n",
    "    \"\"\"Training graph\"\"\"\n",
    "    self.inputs = placeholder(Int32, shape=[-1, self.seq_length])\n",
    "    self.targets = placeholder(Int32, shape=[-1, self.seq_length])\n",
    "    self.predictions = _forward(self, self.inputs)\n",
    "\n",
    "    cost_per_timestep_per_example = [\n",
    "        nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = self.predictions[t],\n",
    "            labels = self.targets[:, t])\n",
    "        for t=1:self.seq_length\n",
    "    ]\n",
    "\n",
    "    # Use reduce_mean over the examples in batch so that we don't need to\n",
    "    # change the learning rate when we change the batch size.\n",
    "    cost_per_timestep = [reduce_mean(cost) for cost in cost_per_timestep_per_example]\n",
    "    \n",
    "    # Total cost is cost averaged over timesteps.\n",
    "    self.cost = mean(cost_per_timestep)\n",
    "\n",
    "    # for annealing the learning rate (not currently used)\n",
    "    self.learning_rate = self.initial_learning_rate * self.decay_factor ^ (self.global_step/self.decay_steps)\n",
    "\n",
    "    self.train_step = train.minimize(train.AdamOptimizer(self.initial_learning_rate, 0, .9, 1e-10, \"Adam\"), self.cost)\n",
    "\n",
    "    \"\"\"Finished creating graph: start session and init vars\"\"\"\n",
    "    self.sess = Session()  \n",
    "\n",
    "    run(self.sess, global_variables_initializer())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_sample (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function run_train(self::RNN, input_chars, target_chars)\n",
    "    \"\"\"Call this to run a train step\"\"\"\n",
    "    cost, lr, _ = run(self.sess, [self.cost, self.learning_rate, self.train_step],\n",
    "                      Dict(self.inputs => input_chars,\n",
    "                           self.targets => target_chars\n",
    "                           )\n",
    "                  )\n",
    "    return cost, lr\n",
    "end\n",
    "\n",
    "function run_sample(self::RNN, n, starter_character, temperature=1.0)\n",
    "    \"\"\"Call this to sample a length-n sequence from the model\"\"\"   \n",
    "    \n",
    "    sampled_chars = [_ for _=1:n]\n",
    "    current_char = starter_character\n",
    "    h = zeros(Float32, 1, self.hidden_size)\n",
    "\n",
    "    for i in 1:n\n",
    "        current_output, h = run(self.sess, [self.next_y, self.next_hidden],\n",
    "                              Dict(self.sample_input_char => current_char,\n",
    "                                   self.sample_input_hidden => h\n",
    "                                   )\n",
    "                              )\n",
    "        probs = exp((current_output-maximum(current_output))/temperature)\n",
    "        probs = probs/sum(probs)\n",
    "        current_char = rand(Categorical(probs[:]))\n",
    "        sampled_chars[i] = current_char\n",
    "    end\n",
    "    return sampled_chars\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100, loss: 2.277777\n",
      "iter "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 10:41:33.696907: E tensorflow/stream_executor/cuda/cuda_driver.cc:405] failed call to cuInit: CUDA_ERROR_UNKNOWN\n",
      "2017-04-26 10:41:33.696937: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: amacrine\n",
      "2017-04-26 10:41:33.696942: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: amacrine\n",
      "2017-04-26 10:41:33.696962: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 381.9.0\n",
      "2017-04-26 10:41:33.696978: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  381.09  Thu Mar 30 20:07:40 PDT 2017\n",
      "GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \n",
      "\"\"\"\n",
      "2017-04-26 10:41:33.696986: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 381.9.0\n",
      "2017-04-26 10:41:33.696989: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 381.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200, loss: 2.057348\n",
      "iter 300, loss: 1.866729\n",
      "iter 400, loss: 1.846181\n",
      "iter 500, loss: 1.848720\n",
      "iter 600, loss: 1.775106\n",
      "iter 700, loss: 1.730525\n",
      "iter 800, loss: 1.844693\n",
      "iter 900, loss: 1.741441\n",
      "iter 1000, loss: 1.726048\n",
      "----\n",
      " Stoying thy sling the word?\n",
      "What nows;\n",
      "Lood turning finds.\n",
      "What, well; ears time,\n",
      "Your graced, if twod\n",
      "In so the kill\n",
      "To see thou tay,\n",
      "Thou with king prothensid?\n",
      "\n",
      "CAMILLO:\n",
      "Trears\n",
      "Would of lose,\n",
      "Who sworn scest that?\n",
      "\n",
      "SLY:\n",
      "For and and\n",
      "besery its;\n",
      "Lrovery Air rumpnet!\n",
      "\n",
      "CAMILLO:\n",
      "STity,\n",
      "And my in.\n",
      "\n",
      "Second, not thine tity in fullous,\n",
      "That its repenty.\n",
      "\n",
      "YORK:\n",
      "My thy to powers\n",
      "Would your retty you.\n",
      "\n",
      "CAMILLA:\n",
      "He oncents: yould tell nake,\n",
      "Dothend would not Kind:\n",
      "And courton you my sines your the limoused \n",
      "----\n",
      "iter 1100, loss: 1.751236\n",
      "iter 1200, loss: 1.729119\n",
      "iter 1300, loss: 1.687824\n",
      "iter 1400, loss: 1.695831\n",
      "iter 1500, loss: 1.650339\n",
      "iter 1600, loss: 1.674415\n",
      "iter 1700, loss: 1.674654\n",
      "iter 1800, loss: 1.705165\n",
      "iter 1900, loss: 1.680999\n",
      "iter 2000, loss: 1.664591\n",
      "----\n",
      " ting thee\n",
      "Thou happy dot be need may and to\n",
      "beauten nay Saught a to yon, so;\n",
      "It prozen,\n",
      "To gone't.\n",
      "\n",
      "Feamorrough book, I the talls hoop'd no are before appethy foundly at may on, too:\n",
      "If thou thy that unsue our he but I daughter, I the souls the so mont 'gain Lonful quest, and thy one the blup;\n",
      "So that's the one not on.\n",
      "My like the charge and can our now then,\n",
      "To humble--father abb spour;\n",
      "I did fual by inch grave an her croaced to would, I country younden consuit virture not our fault no in grorn \n",
      "----\n",
      "iter 2100, loss: 1.609466\n",
      "iter 2200, loss: 1.645457\n",
      "iter 2300, loss: 1.619161\n",
      "iter 2400, loss: 1.610853\n",
      "iter 2500, loss: 1.637321\n",
      "iter 2600, loss: 1.579822\n",
      "iter 2700, loss: 1.643473\n",
      "iter 2800, loss: 1.633406\n",
      "iter 2900, loss: 1.593105\n",
      "iter 3000, loss: 1.606122\n",
      "----\n",
      " ! lost peace, many is of his speed.\n",
      "\n",
      "EXETEUMENBUCHINA:\n",
      "I danges him be vings pluck in the hounds age on.\n",
      "Give a porress I am?\n",
      "\n",
      "First and this slanded, now an assay youse,\n",
      "And should like Vols as is your dance anshuse him no said, sir; which busines say, aid not shall is with hast my brought life in name, but not should kinsman: and shall duty uition as again, and Volsce that not I parchsing you she did is as quard.\n",
      "\n",
      "GLOUCESTER:\n",
      "So grairs good hid mouth bid pains injusan; sir.\n",
      "You musy him saver  \n",
      "----\n",
      "iter 3100, loss: 1.579122\n",
      "iter 3200, loss: 1.622655\n",
      "iter 3300, loss: 1.573298\n",
      "iter 3400, loss: 1.656964\n",
      "iter 3500, loss: 1.581429\n",
      "iter 3600, loss: 1.575174\n",
      "iter 3700, loss: 1.579003\n",
      "iter 3800, loss: 1.583976\n",
      "iter 3900, loss: 1.601256\n",
      "iter 4000, loss: 1.610219\n",
      "----\n",
      " But then in death. death.\n",
      "\n",
      "GREMIO:\n",
      "What that Warwick hath you natuous natureon.\n",
      "\n",
      "PROSPERD:\n",
      "What I have a cold a kingdales the altended a but and heavens the is is say hands of again.\n",
      "\n",
      "ANGELO:\n",
      "I be are at I she sharth: a waste,\n",
      "Day, I caste shupt o' him say as bewbument.\n",
      "\n",
      "BRUTUS:\n",
      "Take word, hear the pawn and by have to be and done.\n",
      "Thus batter state bort again. My death. Say batt having with but they'o: I cause denenanced,\n",
      "As any against in thy diteous of makethin violence found foched be conquia \n",
      "----\n",
      "iter 4100, loss: 1.549971\n",
      "iter 4200, loss: 1.625624\n",
      "iter 4300, loss: 1.548999\n",
      "iter 4400, loss: 1.613413\n",
      "iter 4500, loss: 1.560355\n",
      "iter 4600, loss: 1.585623\n",
      "iter 4700, loss: 1.569290\n",
      "iter 4800, loss: 1.585421\n",
      "iter 4900, loss: 1.616259\n",
      "iter 5000, loss: 1.601457\n",
      "----\n",
      "  woriore overful no porm thou have by are a man's mad have widow years\n",
      "That you soons.\n",
      "\n",
      "CLAUDIO:\n",
      "Pealous\n",
      "Which it tell of thou was your found had the do brithat, who a pray had mustly from think.\n",
      "Now swords my wife:\n",
      "I call of have dirn. For I will my death like him, and arm?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Misday be hunged not with own life you to ill it.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Ay, that have be but thy, this our mine own be\n",
      "Is!\n",
      "Unfrom that King father a pride, and these\n",
      "More unto light thou should your marry shribla \n",
      "----\n",
      "iter 5100, loss: 1.557972\n",
      "iter 5200, loss: 1.552666\n",
      "iter 5300, loss: 1.551041\n",
      "iter 5400, loss: 1.495968\n",
      "iter 5500, loss: 1.588826\n",
      "iter 5600, loss: 1.582633\n",
      "iter 5700, loss: 1.589532\n",
      "iter 5800, loss: 1.576562\n",
      "iter 5900, loss: 1.611785\n",
      "iter 6000, loss: 1.523288\n",
      "----\n",
      " .\n",
      "\n",
      "LARTIUS:\n",
      "Sirruption,\n",
      "Holmost usurprious through thee lay. The backyowor indignifice, indicius; prove,\n",
      "Thousandle.\n",
      "\n",
      "HASTINGS:\n",
      "That, shage, here.\n",
      "\n",
      "KING RICHARD II:\n",
      "Let tided;\n",
      "Sirrah, my being more at as this father's serve,\n",
      "Thou brother.\n",
      "\n",
      "Clown:\n",
      "His thing, but help a draw?\n",
      "\n",
      "KING RICHARD III:\n",
      "Farl out him but minds\n",
      "When hies,\n",
      "Bushiever.\n",
      "\n",
      "CAMILLO:\n",
      "A bamed\n",
      "Supply,\n",
      "We mightness. shumble through\n",
      "othy children by honours\n",
      "Married,\n",
      "As her?\n",
      "Or both time, shall do him.\n",
      "\n",
      "MARCIUS:\n",
      "My much,\n",
      "And proud;\n",
      "And m \n",
      "----\n",
      "iter 6100, loss: 1.557761\n",
      "iter 6200, loss: 1.596519\n",
      "iter 6300, loss: 1.544660\n",
      "iter 6400, loss: 1.515271\n",
      "iter 6500, loss: 1.575240\n",
      "iter 6600, loss: 1.547360\n",
      "iter 6700, loss: 1.565658\n",
      "iter 6800, loss: 1.579122\n",
      "iter 6900, loss: 1.558024\n",
      "iter 7000, loss: 1.534315\n",
      "----\n",
      "  will employful mine I have maid.\n",
      "\n",
      "QUEEN:\n",
      "Here nor there\n",
      "Ur have shall give a morm brave?\n",
      "\n",
      "JULIET:\n",
      "What her;\n",
      "'Twas him are\n",
      "But them\n",
      "That's him thy, for framed banish'd\n",
      "Yours\n",
      "Has\n",
      "murder:\n",
      "But is my lords! make of a\n",
      "call therefore all will brothers, my doubts, that.\n",
      "\n",
      "SICINIUS:\n",
      "Would sake thy silend that have.\n",
      "\n",
      "LUCIO:\n",
      "You cure we my lords.\n",
      "\n",
      "TYBALUS:\n",
      "Longer!\n",
      "Despected thou king, I am pardon have from by thy lands,\n",
      "And me my guiling did their mercass it. That the mean'd sir, for ask,--whip\n",
      "What Tybalt \n",
      "----\n",
      "iter 7100, loss: 1.542481\n",
      "iter 7200, loss: 1.567700\n",
      "iter 7300, loss: 1.590235\n",
      "iter 7400, loss: 1.539472\n",
      "iter 7500, loss: 1.564051\n",
      "iter 7600, loss: 1.580476\n",
      "iter 7700, loss: 1.548331\n",
      "iter 7800, loss: 1.505025\n",
      "iter 7900, loss: 1.571718\n",
      "iter 8000, loss: 1.555822\n",
      "----\n",
      " \n",
      "This:\n",
      "If shall youzil stood.\n",
      "\n",
      "GLOUCESTER:\n",
      "What, which impossure I know have you do a gabbation of all make my so sperate is be my sun\n",
      "With my Mort us bothing see this burlubraged shall pising\n",
      "As the shanda King beautised your grance agon'd my dear and say,\n",
      "Within King so thanks, but bear are lose our thing words alour, sir!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What is as Richard mystimes, as safixs, it as is.\n",
      "\n",
      "POMPEY:\n",
      "Dismio. I voice\n",
      "With and days of kill, is\n",
      "May King thou dies of not if\n",
      "grave as again, I have mak \n",
      "----\n",
      "iter 8100, loss: 1.576607\n",
      "iter 8200, loss: 1.568333\n",
      "iter 8300, loss: 1.570838\n",
      "iter 8400, loss: 1.556497\n",
      "iter 8500, loss: 1.549302\n",
      "iter 8600, loss: 1.573900\n",
      "iter 8700, loss: 1.526566\n",
      "iter 8800, loss: 1.577850\n",
      "iter 8900, loss: 1.599980\n",
      "iter 9000, loss: 1.556494\n",
      "----\n",
      " earkhomal neake and open to of thee strike Lanca justful hands a creat hate hence? good beholdy fair Edwarrer, and her confess and Beppecord carry profan face heart! Who Capidies shalt him be the death\n",
      "A glantach of the far Carrant?\n",
      "\n",
      "CAPULET:\n",
      "Well.\n",
      "\n",
      "CORIOLANUS:\n",
      "Consence! for you dead; of that fair courn there, wrong peril better the prepequarrel not these Coarences care at the father. Go at have to receives you quarred: care if prest full his rear\n",
      "Of come early cross from my troth her. Lehole an \n",
      "----\n",
      "iter 9100, loss: 1.537785\n",
      "iter 9200, loss: 1.562129\n",
      "iter 9300, loss: 1.571034\n",
      "iter 9400, loss: 1.588690\n",
      "iter 9500, loss: 1.506057\n",
      "iter 9600, loss: 1.529764\n",
      "iter 9700, loss: 1.529229\n",
      "iter 9800, loss: 1.511310\n",
      "iter 9900, loss: 1.523616\n",
      "iter 10000, loss: 1.539375\n",
      "----\n",
      " \n",
      "\n",
      "Provost:'\n",
      "Full he roar three the horse muthlys reply perfect renowned his perform thy perish pleasely!\n",
      "\n",
      "CATESBY:\n",
      "I distingt so love is powe present--delieve there him lips all stopp the pot young my wrept the evil see, no speak thy sleep the trike to tyrant,\n",
      "The nape friendshed not brine the present the fection her doom.\n",
      "\n",
      "GLOUCESTER:\n",
      "Now, and play thee deliver thy tone thee not as centry you the shipptizen:\n",
      "She spit crept all the\n",
      "carry well thee he hath gentle,\n",
      "Here where at fines\n",
      "Canst tapes. \n",
      "----\n",
      "iter 10100, loss: 1.582432\n",
      "iter 10200, loss: 1.540570\n",
      "iter 10300, loss: 1.540140\n",
      "iter 10400, loss: 1.536226\n",
      "iter 10500, loss: 1.572756\n",
      "iter 10600, loss: 1.482232\n",
      "iter 10700, loss: 1.542041\n",
      "iter 10800, loss: 1.542181\n",
      "iter 10900, loss: 1.527628\n",
      "iter 11000, loss: 1.585716\n",
      "----\n",
      " ht,\n",
      "As\n",
      "thou'rt\n",
      "them,\n",
      "And is of my heart--fair hanged, and hearth, if your is whose detects,\n",
      "If.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Petackl,\n",
      "How! O, belikely\n",
      "The Lewis Claudies; being you the Duke shame.\n",
      "\n",
      "ROMEO:\n",
      "Where, and lips shrewd drown harmed!\n",
      "What, and is out, Richard's fair dissolumness.\n",
      "Call your will have make you washful should all; good sweet's\n",
      "We's keeeds, my whilst that Alless again; why, sir, shall have.\n",
      "Well, by the man,'d spender their be weigh'd!\n",
      "Now he, here's, if hescrapard.\n",
      "\n",
      "GONZALO:\n",
      "\n",
      "Clown:\n",
      "He the  \n",
      "----\n",
      "iter 11100, loss: 1.526084\n",
      "iter 11200, loss: 1.528751\n",
      "iter 11300, loss: 1.492250\n",
      "iter 11400, loss: 1.555671\n",
      "iter 11500, loss: 1.515424\n",
      "iter 11600, loss: 1.535711\n",
      "iter 11700, loss: 1.586487\n",
      "iter 11800, loss: 1.552967\n",
      "iter 11900, loss: 1.517394\n",
      "iter 12000, loss: 1.571763\n",
      "----\n",
      " s, the roof chiefly.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Where are are temptom of the worlderer, the liege, I wills;\n",
      "So is gracious fierts.'\n",
      "Is this those thou kill'd we were two edier:\n",
      "Mine, for King sperket of Eice, for upon Clart, I do to elefter certaina agree eckingherford, I'll 'ever's prettrun fellowing: to cheerly received I ELIZABETH:\n",
      "Perme.\n",
      "\n",
      "ALONSO:\n",
      "Proceeder-forth.\n",
      "\n",
      "PETRUCHIO:\n",
      "Forgeth or doubt, I go the here,\n",
      "when new without eyes, for fetless'\n",
      "but kiss.\n",
      "\n",
      "LADY ANNE:\n",
      "There, and let complossibboint forth  \n",
      "----\n",
      "iter 12100, loss: 1.591417\n",
      "iter 12200, loss: 1.510554\n",
      "iter 12300, loss: 1.548571\n",
      "iter 12400, loss: 1.541038\n",
      "iter 12500, loss: 1.581977\n",
      "iter 12600, loss: 1.547398\n",
      "iter 12700, loss: 1.523308\n",
      "iter 12800, loss: 1.529365\n",
      "iter 12900, loss: 1.568765\n",
      "iter 13000, loss: 1.539468\n",
      "----\n",
      " fellows.\n",
      "For their to temptures\n",
      "By?\n",
      "Then they yea, in at vison, I'll strength,\n",
      "If thee, with they gives their six, they her life, thy senses you: if him, let's single, the stosed steel-head,\n",
      "As thy incius.\n",
      "\n",
      "ISABULINA:\n",
      "I would,\n",
      "No which as shelow, heaven shall the stinky worthy,\n",
      "Many see your steeds,\n",
      "When, night, cheeges,\n",
      "Neady,\n",
      "Men coverius; their you butcheritious.\n",
      "\n",
      "Vissists;\n",
      "Nothes\n",
      "They.\n",
      "Yet,\n",
      "But stay\n",
      "Of King\n",
      "These they preventues, comes?\n",
      "Mates,\n",
      "With itself.\n",
      "Thy wakes is excusels,\n",
      "Yet owes\n",
      "A q \n",
      "----\n",
      "iter 13100, loss: 1.551364\n",
      "iter 13200, loss: 1.560809\n",
      "iter 13300, loss: 1.548892\n",
      "iter 13400, loss: 1.543073\n",
      "iter 13500, loss: 1.534292\n",
      "iter 13600, loss: 1.507604\n",
      "iter 13700, loss: 1.503831\n",
      "iter 13800, loss: 1.523588\n",
      "iter 13900, loss: 1.537441\n",
      "iter 14000, loss: 1.515878\n",
      "----\n",
      " ow, but sorrow, aupkery.\n",
      "\n",
      "GLOUCESTER:\n",
      "He's him well; shall known dead you will so,--will law,\n",
      "Than be doing.\n",
      "\n",
      "SICINIUS:\n",
      "They himself; blessed of here, or do it office, here entom, metherefore\n",
      "And here and Clight,\n",
      "For lestery!\n",
      "\n",
      "Serform\n",
      "Here; or hereful.\n",
      "\n",
      "LUCIO:\n",
      "Country gots,\n",
      "And leave, you hast, Horks;\n",
      "One hear so sure prayspering be more\n",
      "And son,\n",
      "When devoured here.\n",
      "\n",
      "EDWARD:\n",
      "Not a mighty hangman:\n",
      "Onewels; frost son, herd; child, you know\n",
      "We powerful see.\n",
      "\n",
      "KATHARINA:\n",
      "O, demanding son, as ere mine \n",
      "----\n",
      "iter 14100, loss: 1.531543\n",
      "iter 14200, loss: 1.558701\n",
      "iter 14300, loss: 1.533249\n",
      "iter 14400, loss: 1.517610\n",
      "iter 14500, loss: 1.521474\n",
      "iter 14600, loss: 1.487603\n",
      "iter 14700, loss: 1.556053\n",
      "iter 14800, loss: 1.564776\n",
      "iter 14900, loss: 1.486856\n",
      "iter 15000, loss: 1.525780\n",
      "----\n",
      " him love far grany a loved his amb, forgive; deads Comb and hand;\n",
      "Hark.\n",
      "How Montague, I may I boy high man he not not tongue, it is a should.\n",
      "O hold may a Ricio our let this gentleman the quoth pronounce and budding offend him: I dare.\n",
      "Fir what you have Englain you a glass, whole have his great by no morray\n",
      "Than in heart-stay have, misagua, no name for thy want give shamics it out son,\n",
      "Le't are nose.\n",
      "\n",
      "ANTIGONUS:\n",
      "Receits withal body, and dear than a gentlence own hand a arms, tale is by that now  \n",
      "----\n",
      "iter 15100, loss: 1.524164\n",
      "iter 15200, loss: 1.503551\n",
      "iter 15300, loss: 1.553404\n",
      "iter 15400, loss: 1.509608\n",
      "iter 15500, loss: 1.531653\n",
      "iter 15600, loss: 1.568875\n",
      "iter 15700, loss: 1.510830\n",
      "iter 15800, loss: 1.528168\n",
      "iter 15900, loss: 1.536978\n",
      "iter 16000, loss: 1.516631\n",
      "----\n",
      "  your good ours.\n",
      "\n",
      "WARWICK:\n",
      "Wert\n",
      "knew through so.\n",
      "Awhildren interleven bitternory.\n",
      "\n",
      "MENENIUS:\n",
      "My smally, to be is bring like tiger,\n",
      "Abish, liber, sir, I pouts by wilt duty.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "This right,'\n",
      "Another's hath to methis ny it Scolixation\n",
      "Of there,\n",
      "Are lark, to good-agones!\n",
      "\n",
      "ROMEO:\n",
      "Forbury witory.\n",
      "Fair, and follow to princely.\n",
      "\n",
      "Tail curruptory a mur;\n",
      "Or throus.\n",
      "\n",
      "BENVOLIO:\n",
      "\n",
      "JULIET:\n",
      "Quicess, statuted\n",
      "your senting hell with toward till to missellot point\n",
      "Ascent\n",
      "Lord our state.\n",
      "\n",
      "DUKE OF Edward, \n",
      "----\n",
      "iter 16100, loss: 1.549243\n",
      "iter 16200, loss: 1.557576\n",
      "iter 16300, loss: 1.543183\n",
      "iter 16400, loss: 1.493962\n",
      "iter 16500, loss: 1.538387\n",
      "iter 16600, loss: 1.511869\n",
      "iter 16700, loss: 1.501388\n",
      "iter 16800, loss: 1.528162\n",
      "iter 16900, loss: 1.526839\n",
      "iter 17000, loss: 1.536982\n",
      "----\n",
      " die traitor threen'd or their turn.\n",
      "\n",
      "CORIOLANUS:\n",
      "A do Romago the time it; play porrel liberaln'd!\n",
      "Was some man?\n",
      "Our long there all that the name! Are be son not with spirit be the air.\n",
      "\n",
      "MERCUTIO:\n",
      "With to Barnarding it shall be lord a plant on. What unto living as to time so is enough to sit to-night\n",
      "My night most love as the deliveily call anon, night one to dukedou' that thy own pray that not in woe.\n",
      "\n",
      "AUTOLYCUS:\n",
      "And die?\n",
      "\n",
      "WARWICK:\n",
      "It is not till bed! What locks a truth?\n",
      "\n",
      "GLOUCESTER:\n",
      "How like us \n",
      "----\n",
      "iter 17100, loss: 1.496822\n",
      "iter 17200, loss: 1.533424\n",
      "iter 17300, loss: 1.550323\n",
      "iter 17400, loss: 1.482329\n",
      "iter 17500, loss: 1.512017\n",
      "iter 17600, loss: 1.557115\n",
      "iter 17700, loss: 1.528254\n",
      "iter 17800, loss: 1.544505\n",
      "iter 17900, loss: 1.478311\n",
      "iter 18000, loss: 1.556932\n",
      "----\n",
      " r:\n",
      "The fire.\n",
      "\n",
      "BUCKINGHAM:\n",
      "\n",
      "BENVOLIO:\n",
      "Then\n",
      "Accurse,\n",
      "Myself.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "And thanks;\n",
      "And thou thee\n",
      "And lock is welcome.\n",
      "\n",
      "ROMEO:\n",
      "She true and\n",
      "man.\n",
      "\n",
      "LADY ANNE:\n",
      "If fortuning Veronatch her,\n",
      "Here's suffery man's sad!\n",
      "No, but for the world,\n",
      "Who so suitor,\n",
      "That my daughteded I may rement us.\n",
      "\n",
      "LUCIO:\n",
      "My deseebull?\n",
      "\n",
      "HERMIONE:\n",
      "As tear again\n",
      "And, my hoper,\n",
      "Between's freen;\n",
      "For that leads the way.\n",
      "\n",
      "PETRUCHIO:\n",
      "For stands cannothed to tell,\n",
      "And ladst death'd\n",
      "For you are of noblent,\n",
      "And, for it thing\n",
      "sheel. \n",
      "----\n",
      "iter 18100, loss: 1.549134\n",
      "iter 18200, loss: 1.573128\n",
      "iter 18300, loss: 1.548530\n",
      "iter 18400, loss: 1.506114\n",
      "iter 18500, loss: 1.542590\n",
      "iter 18600, loss: 1.588442\n",
      "iter 18700, loss: 1.558113\n",
      "iter 18800, loss: 1.519749\n",
      "iter 18900, loss: 1.543742\n",
      "iter 19000, loss: 1.506466\n",
      "----\n",
      " y lips I will thou may will, and wife of them of a received proclaim\n",
      "Of Awhill first with with and to man, for him up are good strange double.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Why must poice cexcuse the great long a tend followers:\n",
      "Their gone gut.\n",
      "What paled\n",
      "Would in Lord, let a condition? whom thy could lie of Norship,\n",
      "And high him.\n",
      "\n",
      "Firsp and perforce upon:\n",
      "\n",
      "First in more\n",
      "Why, bid hare propon grievous haught it you doubt startle:\n",
      "Richmond of go can to pinch:\n",
      "But willing.\n",
      "\n",
      "LORD FITZWATER:\n",
      "Now, Signior gentle sungri \n",
      "----\n",
      "iter 19100, loss: 1.504500\n",
      "iter 19200, loss: 1.545620\n",
      "iter 19300, loss: 1.512332\n",
      "iter 19400, loss: 1.493310\n",
      "iter 19500, loss: 1.510125\n",
      "iter 19600, loss: 1.568349\n",
      "iter 19700, loss: 1.571454\n",
      "iter 19800, loss: 1.540017\n",
      "iter 19900, loss: 1.527983\n",
      "iter 20000, loss: 1.506082\n",
      "----\n",
      "  duke of Gredy smile once.\n",
      "\n",
      "Clown:\n",
      "Speak of thou she hatkl set say in pair not us by we wish'd of the cause:\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Yea, she cannry him. What smiles, else you?\n",
      "That it o'clace!\n",
      "I calls doth him go may this drops\n",
      "That upon he did a wall nothing things upon his swere of theal'd star.\n",
      "His we bark,?\n",
      "\n",
      "BENVOLIO:\n",
      "Why buy my conqueld that Arcusa.\n",
      "\n",
      "SICINIUS:\n",
      "What grace see this pite; nor what you hast with his truly mine to can act her sharp a speak his spirit, say your met?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "O \n",
      "----\n",
      "iter 20100, loss: 1.552822\n",
      "iter 20200, loss: 1.518754\n",
      "iter 20300, loss: 1.477429\n",
      "iter 20400, loss: 1.528817\n",
      "iter 20500, loss: 1.578053\n",
      "iter 20600, loss: 1.485355\n",
      "iter 20700, loss: 1.520980\n",
      "iter 20800, loss: 1.522304\n",
      "iter 20900, loss: 1.503463\n",
      "iter 21000, loss: 1.515476\n",
      "----\n",
      " Makes his farewell: as belly, as your sweets disse.\n",
      "Thanks, if all citizens,--\n",
      "Is business\n",
      "In break as less'd;\n",
      "Obseladule your chargess.\n",
      "\n",
      "CLION MOWER:\n",
      "What, if come me!\n",
      "\n",
      "CORIOLANUS:\n",
      "Then clouds.\n",
      "\n",
      "ROMEO:\n",
      "There;\n",
      "If arm'd the priest?\n",
      "\n",
      "GION:\n",
      "It and time?\n",
      "\n",
      "KING RICHARD III:\n",
      "But gold,\n",
      "And rush.\n",
      "\n",
      "AUFIDIUS:\n",
      "Came these,\n",
      "That scarced:\n",
      "Nay, marcius this dance is arroash'd, as grades;\n",
      "Talk:\n",
      "Now a lid shall see his content;\n",
      "Than 'ewle.\n",
      "\n",
      "CAPULET:\n",
      "That, have pay recreation me in like\n",
      "Of his\n",
      "think yourself?\n",
      "\n",
      "IS \n",
      "----\n",
      "iter 21100, loss: 1.526297\n",
      "iter 21200, loss: 1.507179\n",
      "iter 21300, loss: 1.513526\n",
      "iter 21400, loss: 1.523264\n",
      "iter 21500, loss: 1.576921\n",
      "iter 21600, loss: 1.569318\n",
      "iter 21700, loss: 1.533664\n",
      "iter 21800, loss: 1.514604\n",
      "iter 21900, loss: 1.551220\n",
      "iter 22000, loss: 1.515845\n",
      "----\n",
      "  him else empty camplily's curs\n",
      "In mistricle:\n",
      "A gift Phile forgic Clarencial discrawd conform a wail?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "I'll night not entimes and say,\n",
      "To prisplad apess day, but want not!\n",
      "\n",
      "LADY ANNE:\n",
      "By prungle\n",
      "Will no not\n",
      "shieves,\n",
      "And have mittering: them voyam thus dimmond of stop I will minister.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "My Katharina, am Frial most of my far from you are already us a foe knight a sip\n",
      "That harm this bray! who is.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Lift make sins\n",
      "Nursed.\n",
      "\n",
      "YORK:\n",
      "A hard-fury sovereign.\n",
      "\n",
      "DUKE V \n",
      "----\n",
      "iter 22100, loss: 1.535362\n",
      "iter 22200, loss: 1.495013\n",
      "iter 22300, loss: 1.542418\n",
      "iter 22400, loss: 1.526554\n",
      "iter 22500, loss: 1.495799\n",
      "iter 22600, loss: 1.572251\n",
      "iter 22700, loss: 1.531226\n",
      "iter 22800, loss: 1.534239\n",
      "iter 22900, loss: 1.519655\n",
      "iter 23000, loss: 1.543059\n",
      "----\n",
      " iends.\n",
      "\n",
      "WICOMEO:\n",
      "Go, true behience\n",
      "and but call his\n",
      "son!\n",
      "\n",
      "NORTERO:\n",
      "At son,\n",
      "With you.\n",
      "\n",
      "All:\n",
      "Let's thee of both thee, here.\n",
      "\n",
      "NORTERO:\n",
      "On my your.\n",
      "Well, yet you sir, since an in helves, als he hath you.\n",
      "And their\n",
      "curpes the usu! Which he,\n",
      "They come.\n",
      "\n",
      "KING EDWARD:\n",
      "Well.\n",
      "Thou negdingly.\n",
      "\n",
      "WARWICK:\n",
      "Why, the substless and the people is.\n",
      "\n",
      "CAMILLO:\n",
      "The people,\n",
      "These,\n",
      "Even that the grieve theirs castire\n",
      "You shall mistreting blunted\n",
      "me,\n",
      "When deep and sovereign!\n",
      "\n",
      "VOLUMNIA:\n",
      "Heaven is chance; you thought in th \n",
      "----\n",
      "iter 23100, loss: 1.519035\n",
      "iter 23200, loss: 1.551136\n",
      "iter 23300, loss: 1.514631\n",
      "iter 23400, loss: 1.559062\n",
      "iter 23500, loss: 1.545826\n",
      "iter 23600, loss: 1.539465\n",
      "iter 23700, loss: 1.482710\n",
      "iter 23800, loss: 1.508617\n",
      "iter 23900, loss: 1.598909\n",
      "iter 24000, loss: 1.515315\n",
      "----\n",
      " d you, my deserved fellows made of whose muze,\n",
      "And seems me commixt,\n",
      "Yet, to will be of thoughtly's flages gentleman viee\n",
      "For when leaden life sword,\n",
      "And thee, so did lastits more stranger, 'Welvet;\n",
      "Women which swords, right,\n",
      "Let hell.\n",
      "\n",
      "First:\n",
      "Prove we ever day;\n",
      "I comes more two; birry if\n",
      "seefer,\n",
      "Weep-say, you wrong well, Clifford!\n",
      "For laments,\n",
      "And fircess\n",
      "As see fair I.\n",
      "\n",
      "KING EDWARD IV:\n",
      "No?\n",
      "\n",
      "Lo: more whet to?\n",
      "\n",
      "PETRUCHIO:\n",
      "My vastly\n",
      "And from my sorged few beseed,\n",
      "For stort-blows it hopest with co \n",
      "----\n",
      "iter 24100, loss: 1.585886\n",
      "iter 24200, loss: 1.506122\n",
      "iter 24300, loss: 1.513250\n",
      "iter 24400, loss: 1.495838\n",
      "iter "
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "fid = open(\"shakespeare.txt\") # should be simple plain text file\n",
    "data = readstring(fid)\n",
    "close(fid)\n",
    "chars = unique(data)\n",
    "data_size, vocab_size = length(data), length(chars)\n",
    "@printf \"data has %d characters, %d unique.\\n\" data_size vocab_size\n",
    "char_to_ix = Dict(chars[i] => i for i=1:vocab_size )\n",
    "ix_to_char = Dict(i => chars[i] for i=1:vocab_size )\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "embedding_size = 32 # size of embedding\n",
    "hidden_size = 256 # size of hidden layers of neurons\n",
    "seq_length = 50 # number of steps to unroll the RNN for\n",
    "initial_learning_rate = 1e-2\n",
    "decay_steps = 500.0\n",
    "decay_factor = 0.9\n",
    "sample_len = 500\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "n_train_steps = 100000\n",
    "\n",
    "# model parameters\n",
    "rnn = RNN(batch_size, embedding_size, hidden_size, vocab_size, \n",
    "          seq_length, initial_learning_rate, decay_steps, decay_factor)\n",
    "\n",
    "_init_params(rnn)\n",
    "_build_graph(rnn)\n",
    "\n",
    "loss = zeros(n_train_steps)\n",
    "\n",
    "# prepare inputs and target values\n",
    "inputs = zeros(Int32, batch_size, seq_length)\n",
    "targets = zeros(Int32, batch_size, seq_length)\n",
    "    \n",
    "for n = 1:n_train_steps\n",
    "    for i = 1:batch_size\n",
    "        # randomly index into the data for each example in batch\n",
    "        random_index = Int32(ceil(rand() * (data_size - seq_length)))\n",
    "        inputs[i, :] = [char_to_ix[ch] for ch in data[random_index:random_index+seq_length-1]]\n",
    "        targets[i, :] = [char_to_ix[ch] for ch in data[random_index+1:random_index+seq_length]]\n",
    "    end\n",
    "\n",
    "    loss[n], lr = run_train(rnn, inputs, targets)\n",
    "\n",
    "    # print progress\n",
    "    if n % 100 == 0\n",
    "        @printf(\"iter %d, loss: %f\\n\", n, loss[n])\n",
    "    end\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0\n",
    "        sample_ix = run_sample(rnn, sample_len, inputs[1, 1], 1.0)\n",
    "        txt = string(map(string,[ix_to_char[ix] for ix in sample_ix])...)\n",
    "        @printf(\"----\\n %s \\n----\\n\", txt)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What is the cost after 10,000 training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try sampling with high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_ix = run_sample(rnn, sample_len, inputs[1, 1], 100.0)\n",
    "txt = string(map(string,[ix_to_char[ix] for ix in sample_ix])...)\n",
    "@printf(\"----\\n %s \\n----\\n\", txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now with very low temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_ix = run_sample(rnn, sample_len, inputs[1, 1], 0.001)\n",
    "txt = string(map(string,[ix_to_char[ix] for ix in sample_ix])...)\n",
    "@printf(\"----\\n %s \\n----\\n\", txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do the samples qualitatively change? What effect does temperature have, mathematically, on the output distribution?\n",
    "In the softmax function with a temperature T, we use e^(x_i / T) instead of e^(x_i):\n",
    "output_i = e^(x_i / T) / Z, where Z is the normalizer: Z = sum_j e^(x_j / T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert answer here*\n",
    "\n",
    "Type Markdown and LaTeX:  $\\alpha^2$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
