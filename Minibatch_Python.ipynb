{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation learning for multilayer perceptron\n",
    "# with minibatches\n",
    "\n",
    "    @author      Qipeng Liu\n",
    "    @rev.date    2017/02/22\n",
    "\n",
    "Python 2.7 port of Minibatch.ipynb Julia code by Prof. Sebastian Seung\n",
    "\n",
    "- Note: In this code, label 0 has index 0. (Python indexing starts from 0, Julia from 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):                # logistic function activation (replace to define your own activation function)\n",
    "    return np.tanh(x)\n",
    "def df(y):               # derivative of f composed with inverse of f\n",
    "    return 1 - np.multiply(y, y)\n",
    "\n",
    "n0 = 784                 # widths of layers\n",
    "n1 = 200\n",
    "n2 = 100\n",
    "n3 = 10  \n",
    "\n",
    "eta = 0.01               # learning rate parameter\n",
    "epsinit = 0.01           # magnitude of initial conditions for synaptic weights\n",
    "\n",
    "# two fully connected synaptic layers\n",
    "W1 = epsinit*np.random.randn(n1,n0)\n",
    "W2 = epsinit*np.random.randn(n2,n1)\n",
    "W3 = epsinit*np.random.randn(n3,n2)\n",
    "\n",
    "# biases\n",
    "b1 = epsinit*np.random.randn(n1, 1)\n",
    "b2 = epsinit*np.random.randn(n2, 1)\n",
    "b3 = epsinit*np.random.randn(n3, 1)\n",
    "\n",
    "tmax = 600000             # maximum number of learning updates\n",
    "tshow = 1000              # how often to pause for visualization\n",
    "errsq = np.zeros(tmax)\n",
    "errcl = np.zeros(tmax)\n",
    "errclvalidate = np.zeros(tmax / tshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess training set\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"data\")\n",
    "\n",
    "train = mnist.data[:60000]\n",
    "train = np.divide(train, 255.0)\n",
    "trainlabel = mnist.target[:60000]\n",
    "\n",
    "# separate out validation set\n",
    "mtotal = len(train)             # number of examples in training set\n",
    "mvalidate = 10000               # desired size of validation set\n",
    "mtrain = mtotal - mvalidate     # remaining examples will be the new training set\n",
    "np.random.seed(495)             # seed the random number generator so that validation set is reproducible\n",
    "ind = np.random.permutation(range(mtotal))\n",
    "validate = [train[i] for i in ind[:mvalidate]]\n",
    "validatelabels = [trainlabel[i] for i in ind[:mvalidate]]\n",
    "train = [train[i] for i in ind[mvalidate:]]\n",
    "trainlabels = [trainlabel[i] for i in ind[mvalidate:]]\n",
    "\n",
    "batchsize = 32     # minibatch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-586d8cd573ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# error computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0merrsq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0merrcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdelta3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "fig_size = (12,12)                           # you may need to change the numbers to fit your screen\n",
    "for t in xrange(tmax):\n",
    "    # generate random samples from train set\n",
    "    batchindices = [int(np.floor(mtrain * np.random.rand())) for i in xrange(batchsize)] \n",
    "    x0 = np.zeros([n0, batchsize])\n",
    "    for i, j in zip(range(batchsize), batchindices):\n",
    "        x0[:, i] = train[j]\n",
    "    \n",
    "    y = -np.ones([n3, batchsize])\n",
    "    for i, j in zip(range(batchsize), batchindices):\n",
    "        y[int(trainlabels[j]), i] = 1.0\n",
    "    \n",
    "    B1 = np.repeat(b1, batchsize).reshape([n1, batchsize])\n",
    "    B2 = np.repeat(b2, batchsize).reshape([n2, batchsize])\n",
    "    B3 = np.repeat(b3, batchsize).reshape([n3, batchsize])\n",
    "    \n",
    "    # forward pass   \n",
    "    x1 = f(np.dot(W1,x0)+B1)\n",
    "    x2 = f(np.dot(W2,x1)+B2)\n",
    "    x3 = f(np.dot(W3,x2)+B3)\n",
    "    \n",
    "    # error computation\n",
    "    errsq[t] = sum(sum(np.power((y-x3), 2))) / batchsize\n",
    "    errcl[t] = sum([float(np.argmax(x3[:, i]) != int(trainlabels[j])) for i, j in zip(range(batchsize), batchindices)]) / batchsize\n",
    "    delta3 = np.multiply((y-x3),df(x3))\n",
    "    \n",
    "    # backward pass\n",
    "    delta2 = np.multiply(np.dot(W3.T, delta3), df(x2))\n",
    "    delta1 = np.multiply(np.dot(W2.T, delta2), df(x1))\n",
    "\n",
    "    # learning updates\n",
    "    W3 += eta / batchsize * np.dot(delta3, x2.T)\n",
    "    W2 += eta / batchsize * np.dot(delta2, x1.T)\n",
    "    W1 += eta / batchsize * np.dot(delta1, x0.T)\n",
    "    b3 += eta / batchsize * np.sum(delta3, axis=1).reshape(n3, 1)\n",
    "    b2 += eta / batchsize * np.sum(delta2, axis=1).reshape(n2, 1)\n",
    "    b1 += eta / batchsize * np.sum(delta1, axis=1).reshape(n1, 1)\n",
    "\n",
    "    if t % tshow == 0 and t > 0:    # visualization every tshow steps\n",
    "        avgerrsq = np.sum(errsq[: t].reshape(t / tshow, tshow), axis=1).reshape(t / tshow, 1) / tshow\n",
    "        avgerrcl = np.sum(errcl[: t].reshape(t / tshow, tshow), axis=1).reshape(t / tshow, 1) / tshow\n",
    "        \n",
    "        # compute error on validation set\n",
    "        x0 = np.zeros([n0, mvalidate])\n",
    "        for i in range(mvalidate):\n",
    "            x0[:, i] = validate[i]\n",
    "        B1 = np.repeat(b1, mvalidate).reshape([n1, mvalidate])\n",
    "        B2 = np.repeat(b2, mvalidate).reshape([n2, mvalidate])\n",
    "        B3 = np.repeat(b3, mvalidate).reshape([n3, mvalidate])\n",
    "        x1 = f(np.dot(W1,x0)+B1)\n",
    "        x2 = f(np.dot(W2,x1)+B2)\n",
    "        x3 = f(np.dot(W3,x2)+B3)\n",
    "        errclvalidate[t / tshow - 1] = sum([float(np.argmax(x3[:, i]) != int(validatelabels[i])) for i in xrange(mvalidate)]) / mvalidate\n",
    "        \n",
    "        # plot figures\n",
    "        fig = plt.figure(figsize=fig_size)\n",
    "        gs = gridspec.GridSpec(2, 3, wspace=0.3, hspace=0.3)\n",
    "\n",
    "        ax = fig.add_subplot(gs[0,0])\n",
    "        ax.plot(range(1, t / tshow + 1), avgerrsq, label=\"squared\")\n",
    "        ax.set_ylabel(\"sq err\")\n",
    "        ax.set_ylim([0.001,4])\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel(\"x{} minibatches\".format(tshow))\n",
    "        ax.grid()\n",
    "        \n",
    "        ax = fig.add_subplot(gs[0,1])\n",
    "        ax.plot(range(1, t / tshow + 1), avgerrcl, label=\"train\")\n",
    "        ax.plot(range(1, t / tshow + 1), errclvalidate[: t / tshow], label=\"validation\")\n",
    "        ax.set_ylabel(\"class err\")\n",
    "        ax.set_ylim([0.001,1])\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title(\"t={}\".format(t))\n",
    "        ax.set_xlabel(\"x{} minibatches\".format(tshow))\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "        \n",
    "        ax = fig.add_subplot(gs[0,2])\n",
    "        ax.plot(range(1, t / tshow + 1), avgerrcl, label=\"train\")\n",
    "        ax.plot(range(1, t / tshow + 1), errclvalidate[: t / tshow], label=\"validation\")\n",
    "        ax.set_ylabel(\"class err\")\n",
    "        ax.set_ylim([0.0, 0.1])\n",
    "        ax.set_title(\"t={}\".format(t))\n",
    "        ax.set_xlabel(\"x{} minibatches\".format(tshow))\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "        \n",
    "        ax = fig.add_subplot(gs[1, 0])\n",
    "        ax.hist(x1.flatten())\n",
    "        ax.set_xlabel(\"x1\")\n",
    "        ax.grid()\n",
    "        \n",
    "        ax = fig.add_subplot(gs[1, 1])\n",
    "        ax.hist(x2.flatten())\n",
    "        ax.set_xlabel(\"x2\")\n",
    "        ax.grid()\n",
    "        \n",
    "        ax = fig.add_subplot(gs[1, 2])\n",
    "        ax.hist(x3.flatten())\n",
    "        ax.set_xlabel(\"x3\")\n",
    "        ax.grid()\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        time.sleep(0.01)\n",
    "        display.clear_output(wait=True)\n",
    "        fig.clf()\n",
    "        plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
