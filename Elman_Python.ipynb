{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Character-level RNN model\n",
    "\n",
    "Author: Alex Beatson\n",
    "\n",
    "Data I/O adapted from Andrej Karpathy's CharRNN gist: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "See his blog post for some fun applications of RNNs: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "BSD License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Design notes:\n",
    "\n",
    "All TensorFlow computation is wrapped in the RNN class.\n",
    "All non-TF computation (except feeding inputs) happens outside the class.\n",
    "\n",
    "\"private\" class methods preceeded by underscore (e.g. _init_params, _rnn_step) accessed within RNN.\n",
    "\n",
    "\"public\" class methods without underscore (run_train, run_sample) accessed outside RNN.\n",
    "\n",
    "All placeholders are defined in _build_graph, and all placeholder values are fed in by public methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Student note:\n",
    "\n",
    "You should focus on understanding the RNN methods _init_params, _rnn_step, and _forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_size, hidden_size, vocab_size, seq_length,\n",
    "                 learning_rate, decay_steps, decay_factor, max_grad, sample_len):\n",
    "        ''' Set the hyperparameters and define the computation graph.\n",
    "        '''\n",
    "\n",
    "        ''' hyperparameters '''\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size # number of chars in vocab\n",
    "        self.seq_length = seq_length # number of steps to unroll the RNN for\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_factor = decay_factor\n",
    "        self.max_grad = max_grad\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        # this var keeps track of the train steps within the RNN\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        ''' create vars and graph '''\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        '''Create the model parameters'''\n",
    "        \n",
    "        # We learn an embedding for each character jointly with the other model params\n",
    "        self.embedding = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_size],\n",
    "                                                      mean=0, stddev=0.2))\n",
    "\n",
    "        self.U = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "            \n",
    "        self.W = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bh = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "\n",
    "        self.V = tf.Variable(tf.random_normal([self.hidden_size, self.vocab_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.by = tf.Variable(tf.zeros([1, self.vocab_size]))\n",
    "\n",
    "\n",
    "    def _rnn_step(self, x, h):\n",
    "        '''Performs RNN computation for one timestep:\n",
    "        takes a previous x and h, and computes the next x and h.\n",
    "        \n",
    "        In practical applications, you should almost always use TensorFlow's built-in RNN cells,\n",
    "        from tf.contrib.rnn. However for teaching purposes we are writing the RNN from scratch here.\n",
    "        '''\n",
    "        \n",
    "        h = tf.nn.sigmoid(tf.matmul(x, self.U) + tf.matmul(h, self.W) + self.bh)\n",
    "        y = tf.matmul(h, self.V) + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    \n",
    "    def _forward(self, inputs):\n",
    "        '''Performs the forward pass for all timesteps in a sequence.\n",
    "        '''\n",
    "        # Create list to hold y\n",
    "        y = [_ for _ in range(self.seq_length)]\n",
    "\n",
    "        # Create zero-d initial hidden state\n",
    "        h = tf.zeros([self.batch_size, self.hidden_size])\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            x = tf.nn.embedding_lookup(self.embedding, inputs[:, t])\n",
    "            y[t], h = self._rnn_step(x, h)\n",
    "\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def _sample_one(self, input_character, input_hidden, temperature):\n",
    "        '''Sample the single next character in a sequence.\n",
    "\n",
    "        We can use this to sample sequences of any length w/o having to alter\n",
    "        the tensorflow graph.'''\n",
    "\n",
    "        # We expand dims because tf expects a batch\n",
    "        character = tf.expand_dims(input_character, 0)\n",
    "\n",
    "        # Get the embedding for the input character\n",
    "        x = tf.nn.embedding_lookup(self.embedding, character)\n",
    "\n",
    "        # Perform the RNN look up\n",
    "        y, h = self._rnn_step(x, input_hidden)\n",
    "\n",
    "        # Dividing the unnormalized probabilities by the temperature before \n",
    "        # tf.multinomial is equivalent to adding temperature to a softmax\n",
    "        # before sampling\n",
    "        y_temperature = y / temperature\n",
    "\n",
    "        # We use tf.squeeze to remove the unnecessary [batch, num_samples] dims\n",
    "        # We do not manually softmax - tf.multinomial softmaxes the tensor we pass it\n",
    "        next_sample = tf.squeeze(tf.multinomial(y_temperature, 1))\n",
    "\n",
    "        return next_sample, h\n",
    "\n",
    "\n",
    "    def _build_graph(self):\n",
    "        '''Build the computation graphs for training and sampling.\n",
    "\n",
    "        All placeholders are defined in this method.'''\n",
    "\n",
    "\n",
    "        '''Sampling graph'''\n",
    "        self.sample_input_char = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.sample_input_hidden = tf.placeholder(dtype=tf.float32, shape=[1, self.hidden_size])\n",
    "\n",
    "        self.temperature = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        self.next_sample, self.next_hidden = self._sample_one(\n",
    "            self.sample_input_char, self.sample_input_hidden, self.temperature)\n",
    "\n",
    "\n",
    "        '''Training graph'''\n",
    "        self.inputs = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.targets = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.predictions = self._forward(self.inputs)\n",
    "\n",
    "        cost_per_timestep_per_example = [\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.predictions[t],\n",
    "                    labels=self.targets[:, t])\n",
    "                for t in range(self.seq_length)\n",
    "        ]\n",
    "\n",
    "        # Use reduce_mean over the examples in batch so that we don't need to\n",
    "        # change the learning rate when we change the batch size.\n",
    "        cost_per_timestep = [tf.reduce_mean(cost) for cost in cost_per_timestep_per_example]\n",
    "\n",
    "        # Total cost is cost summed over timesteps.\n",
    "        self.cost = tf.reduce_sum(cost_per_timestep)\n",
    "\n",
    "        # Decay the learning rate according to a schedule.\n",
    "        self.learning_rate = tf.train.exponential_decay(self.initial_learning_rate,\n",
    "                                                        self.global_step,\n",
    "                                                        self.decay_steps,\n",
    "                                                        self.decay_factor)\n",
    "        \n",
    "        self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(\n",
    "            self.cost, global_step=self.global_step)\n",
    "\n",
    "\n",
    "        '''Finished creating graph: start session and init vars'''\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def run_train(self, input_chars, target_chars):\n",
    "        '''Call this from outside the class to run a train step'''\n",
    "        cost, lr, _ = self.sess.run([self.cost, self.learning_rate, self.train_step],\n",
    "                                   feed_dict={\n",
    "                                       self.inputs: input_chars,\n",
    "                                       self.targets: target_chars\n",
    "                                   })\n",
    "        return cost, lr\n",
    "\n",
    "\n",
    "    def run_sample(self, n, starter_character, temperature=1.0):\n",
    "        '''Call this from outside the class to sample a length-n sequence from the model'''\n",
    "\n",
    "        sampled_chars = [_ for _ in range(n)]\n",
    "        current_char = starter_character\n",
    "        h = np.zeros([1, self.hidden_size])\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            current_char, h = self.sess.run(\n",
    "                [self.next_sample, self.next_hidden],\n",
    "                feed_dict={\n",
    "                    self.sample_input_char: current_char,\n",
    "                    self.sample_input_hidden: h,\n",
    "                    self.temperature: temperature})\n",
    "\n",
    "            sampled_chars[i] = current_char\n",
    "\n",
    "        return sampled_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n",
      "iter 0, loss: 631.948120, learning rate: 0.000100\n",
      "----\n",
      " ,:Iv,lUV,vP;EaPvIlmP&PvPIE;Dl;;l,FPPElUvEmllE,MEMv,lyPDEv&bEl,&&,lEUlIelPID?llllEePgGlufPvsDvU;aPPj-Us;:lEDdE&PPl,alIMlIPD&,r&EXDMMEdUMylDbIvjvUElg:P:lEUP'llIH;EPEPlvXEHlEIl;jjbIl&v&P&PlUfMPvEUvvlBuMvIPalIPvxl&VynPPPlDaPll;,&?g&PPlPUDlvv;vDl'HlDMM,cv&ll&PPUlbvXEPPEP;,,MUI:vEX&BvvBMlxvl'l';XqY&IP&UjlEPvlflE,lIIU:HXa;I!,l;llPHnE&aUEll:xU-zPPlMuPFP,EXDOPvE;,jXlluvXUxXvPl,&lM;,yEEl-'PPUPIPuPljHXlPvjMl3,EBllPPHP:ll&WX,lglPU?P:,DliMMKuP:MW&PEPyH&lMuPldD,&lMlE;lDl;l'jV&PI3gPPMl,M'llXTv3PP;V?EPjExMlyP&U \n",
      "----\n",
      "iter 100, loss: 346.171997, learning rate: 0.000100\n",
      "iter 200, loss: 300.678925, learning rate: 0.000100\n",
      "iter 300, loss: 282.154327, learning rate: 0.000100\n",
      "iter 400, loss: 265.376404, learning rate: 0.000100\n",
      "iter 500, loss: 252.267639, learning rate: 0.000100\n",
      "iter 600, loss: 250.253036, learning rate: 0.000100\n",
      "iter 700, loss: 240.579346, learning rate: 0.000100\n",
      "iter 800, loss: 234.779083, learning rate: 0.000100\n",
      "iter 900, loss: 233.032806, learning rate: 0.000100\n",
      "iter 1000, loss: 231.850586, learning rate: 0.000100\n",
      "----\n",
      " tland serssatd fostuin thithon fid sno, nove warmehe Lu-pUTht wWAP;ise st lo and hithen buthingd thos merd is, ut merd tredana beap\n",
      "\n",
      "MANE bEbtto chy no mfot, wodsste, the abd:\n",
      "I aals.\n",
      "Bl Rhir cherle,\n",
      "TordwSit tiag\n",
      "Whis mar!\n",
      "PNDO: youss infdret no pille\n",
      "Ars ulT.\n",
      "SR brome e arow'\n",
      "Wit fe hto Doops thas sicrhe cakt \n",
      "! co he port ou mser have soU's s?rdRe noadU:\n",
      "Ous ayk\n",
      "dssie,\n",
      "A\n",
      "Ard\n",
      "WUEELANUII :\n",
      "HIprave.\n",
      "\n",
      "SCrERSRe:\n",
      "Is yould,\n",
      "\n",
      "ury youl ins ald htrefo?X\n",
      "\n",
      "Ror Oour hf ar out luts; in m.\n",
      "Mer ard mer thor  \n",
      "----\n",
      "iter 1100, loss: 228.079819, learning rate: 0.000100\n",
      "iter 1200, loss: 223.855255, learning rate: 0.000100\n",
      "iter 1300, loss: 220.594269, learning rate: 0.000100\n",
      "iter 1400, loss: 225.654327, learning rate: 0.000100\n",
      "iter 1500, loss: 222.801102, learning rate: 0.000100\n",
      "iter 1600, loss: 215.776749, learning rate: 0.000100\n",
      "iter 1700, loss: 216.421173, learning rate: 0.000100\n",
      "iter 1800, loss: 214.049591, learning rate: 0.000100\n",
      "iter 1900, loss: 216.512177, learning rate: 0.000100\n",
      "iter 2000, loss: 215.263062, learning rate: 0.000100\n",
      "----\n",
      "  miry somere afte willndiverking if Hessily th the fioutina!\n",
      "Sop are mbenote.\n",
      "\n",
      "TLOUK:\n",
      "Fre no an al I matle ve thes thy chlisl flome poke beseene.\n",
      "\n",
      "POMUMn conkuse fan wirt you preatr to lefe shenke woreact dearell.\n",
      "\n",
      "Thence you thee\n",
      "se?\n",
      "\n",
      "Thengots wit sispard!\n",
      "Beve my chel ast il itis nowry yot tave your preat to fore;\n",
      "A.\n",
      "\n",
      "CUbANDUCHTlit thele will wokroand cob toor\n",
      "Dour, worther dor my that in bhan An and foverzerveaing owe feras louth to lord whon hon, shish. will,\n",
      "Ard!\n",
      "MGLIUVIOCI gAMPENNNCUCENrkO \n",
      "----\n",
      "iter 2100, loss: 209.186554, learning rate: 0.000100\n",
      "iter 2200, loss: 217.287399, learning rate: 0.000100\n",
      "iter 2300, loss: 211.120041, learning rate: 0.000100\n",
      "iter 2400, loss: 205.779419, learning rate: 0.000100\n",
      "iter 2500, loss: 210.681152, learning rate: 0.000100\n",
      "iter 2600, loss: 208.337906, learning rate: 0.000100\n",
      "iter 2700, loss: 210.062561, learning rate: 0.000100\n",
      "iter 2800, loss: 205.969559, learning rate: 0.000100\n",
      "iter 2900, loss: 199.420441, learning rate: 0.000100\n",
      "iter 3000, loss: 203.754669, learning rate: 0.000100\n",
      "----\n",
      "  his lof Herinds,\n",
      "Myou thiedse:\n",
      "\n",
      "IO whel ire,\n",
      "Geat sur buesil, are ffoins toon the mpaad\n",
      "ford an and sorits hit turd bide frake past'ther:\n",
      "And,\n",
      "Whar ake a op Sedeath yound.\n",
      "\n",
      "PAMURY:\n",
      "I IVof thef ofeth m atr Lemesplangt ey thy yourouge at,\n",
      "Goso sot ta will 't Vansut inos,\n",
      "How, strenu priches fooffrsy thId selk ye? mevere tore woal beaping of os oled ours gattunt for thaurd you do thee stoy trotf.\n",
      "\n",
      "ME VING:\n",
      "Wey nie shand ce lond w; cFrelce, your an hily wiot me than spart'erio, thine blliput oupy y \n",
      "----\n",
      "iter 3100, loss: 201.830734, learning rate: 0.000100\n",
      "iter 3200, loss: 210.559357, learning rate: 0.000100\n",
      "iter 3300, loss: 196.481201, learning rate: 0.000100\n",
      "iter 3400, loss: 200.295212, learning rate: 0.000100\n",
      "iter 3500, loss: 203.969589, learning rate: 0.000100\n",
      "iter 3600, loss: 201.623398, learning rate: 0.000100\n",
      "iter 3700, loss: 199.058899, learning rate: 0.000100\n",
      "iter 3800, loss: 199.985611, learning rate: 0.000100\n",
      "iter 3900, loss: 194.782166, learning rate: 0.000100\n",
      "iter 4000, loss: 197.129196, learning rate: 0.000100\n",
      "----\n",
      " e e no likers and him hy chatlaurdord:\n",
      "Woll hatr we'll I hant shim meder the they ure,\n",
      "Se comeepl Waugh hor;\n",
      "Or styyenseg bedtsu,\n",
      "Ay to bel's. EDach mere hum highting\n",
      "Out in dupatais cimenee offzanc,\n",
      "Myoubl ill of whaw you have sean that in, ther he hym, is bay, and senosedad, an your so.\n",
      "\n",
      "POFIUY:\n",
      "Anoonoung shound,\n",
      "That bod mot ettrouncu!\n",
      "Wish medefral'st us lotse iry dowrufeo dalg onter-mainate\n",
      "Ass,\n",
      "Thete a fore threce\n",
      "Kn thid\n",
      "the cajust in whour weut gre, here am soofy one y tilloys, of yo tho \n",
      "----\n",
      "iter 4100, loss: 194.047623, learning rate: 0.000100\n",
      "iter 4200, loss: 203.227310, learning rate: 0.000100\n",
      "iter 4300, loss: 191.817154, learning rate: 0.000100\n",
      "iter 4400, loss: 193.875092, learning rate: 0.000100\n",
      "iter 4500, loss: 195.525452, learning rate: 0.000100\n"
     ]
    }
   ],
   "source": [
    "'''Train and sample from our model'''\n",
    "\n",
    "# data I/O\n",
    "data = open('shakespeare.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "embedding_size = 256 # size of embedding\n",
    "hidden_size = 256 # size of hidden layers of neurons\n",
    "seq_length = 100 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-4\n",
    "max_grad = 5\n",
    "decay_steps = 10000\n",
    "decay_factor = 0.99\n",
    "sample_len = 500\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "n_train_steps = 100000\n",
    "\n",
    "# model parameters\n",
    "rnn = RNN(batch_size, embedding_size, hidden_size, vocab_size, \n",
    "          seq_length, learning_rate, decay_steps, decay_factor, \n",
    "          max_grad, sample_len)\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "for n in range(n_train_steps):\n",
    "    \n",
    "    # prepare inputs \n",
    "    inputs = np.empty([batch_size, seq_length])\n",
    "    targets = np.empty([batch_size, seq_length])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # randomly index into the data for each example in batch\n",
    "        random_index = int(np.random.rand() * (data_size - seq_length - 1))\n",
    "        inputs[i, :] = [char_to_ix[ch] for ch in data[random_index:random_index+seq_length]]\n",
    "        targets[i, :] = [char_to_ix[ch] for ch in data[random_index+1:random_index+seq_length+1]]\n",
    "        \n",
    "    loss, lr = rnn.run_train(inputs, targets)\n",
    "    \n",
    "    # print progress\n",
    "    if n % 100 == 0: \n",
    "        print 'iter %d, loss: %f, learning rate: %f' % (n, loss, lr) \n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 1.0)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is the cost after 25,000 train steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Let's try sampling with high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 100)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Now with very low temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 0.001)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How do the samples qualitatively change? What effect does temperature have, mathematically, on the output distribution?\n",
    "\n",
    "In the softmax function with a temperature T, we use e^(x_i / T) instead of e^(x_i):\n",
    "\n",
    "output_i = e^(x_i / T) / Z,\n",
    "where Z is the normalizer: Z = sum_j e^(x_j / T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
