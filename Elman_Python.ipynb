{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Character-level RNN model\n",
    "\n",
    "Author: Alex Beatson\n",
    "\n",
    "Data I/O adapted from Andrej Karpathy's CharRNN gist: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "See his blog post for some fun applications of RNNs: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "BSD License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Design notes:\n",
    "\n",
    "TensorFlow computation wrapped in the RNN class.\n",
    "\n",
    "Non-TF computation (except feeding inputs) happens outside the class.\n",
    "\n",
    "\"private\" class methods preceeded by underscore (e.g. _init_params, _rnn_step) accessed within RNN.\n",
    "\n",
    "\"public\" class methods without underscore (run_train, run_sample) accessed outside RNN.\n",
    "\n",
    "Placeholders are defined in _build_graph, placeholder values are fed in by public methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Student note:\n",
    "\n",
    "You should focus on understanding the RNN methods _init_params, _rnn_step, and _forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_size, hidden_size, vocab_size, seq_length,\n",
    "                 learning_rate, decay_steps, decay_factor, sample_len):\n",
    "        ''' Set the hyperparameters and define the computation graph.\n",
    "        '''\n",
    "\n",
    "        ''' hyperparameters '''\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size # number of chars in vocab\n",
    "        self.seq_length = seq_length # number of steps to unroll the RNN for\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_factor = decay_factor\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        # this var keeps track of the train steps within the RNN\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        ''' create vars and graph '''\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "        self._build_graph()\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        '''Create the model parameters'''\n",
    "        \n",
    "        # We learn an embedding for each character jointly with the other model params\n",
    "        self.embedding = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_size],\n",
    "                                                      mean=0, stddev=0.2))\n",
    "\n",
    "        self.U = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "            \n",
    "        self.W = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bh = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "\n",
    "        self.V = tf.Variable(tf.random_normal([self.hidden_size, self.vocab_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.by = tf.Variable(tf.zeros([1, self.vocab_size]))\n",
    "\n",
    "\n",
    "    def _rnn_step(self, x, h):\n",
    "        '''Performs RNN computation for one timestep:\n",
    "        takes a previous x and h, and computes the next x and h.\n",
    "        \n",
    "        In practical applications, you should almost always use TensorFlow's built-in RNN cells,\n",
    "        from tf.contrib.rnn. However for teaching purposes we are writing the RNN from scratch here.\n",
    "        '''\n",
    "        \n",
    "        h = tf.nn.sigmoid(tf.matmul(x, self.U) + tf.matmul(h, self.W) + self.bh)\n",
    "        y = tf.matmul(h, self.V) + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    \n",
    "    def _forward(self, inputs):\n",
    "        '''Performs the forward pass for all timesteps in a sequence.\n",
    "        '''\n",
    "        # Create list to hold y\n",
    "        y = [_ for _ in range(self.seq_length)]\n",
    "\n",
    "        # Create zero-d initial hidden state\n",
    "        h = tf.zeros([self.batch_size, self.hidden_size])\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            x = tf.nn.embedding_lookup(self.embedding, inputs[:, t])\n",
    "            y[t], h = self._rnn_step(x, h)\n",
    "\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def _sample_one(self, input_character, input_hidden, temperature):\n",
    "        '''Sample the single next character in a sequence.\n",
    "\n",
    "        We can use this to sample sequences of any length w/o having to alter\n",
    "        the tensorflow graph.'''\n",
    "\n",
    "        # We expand dims because tf expects a batch\n",
    "        character = tf.expand_dims(input_character, 0)\n",
    "\n",
    "        # Get the embedding for the input character\n",
    "        x = tf.nn.embedding_lookup(self.embedding, character)\n",
    "\n",
    "        # Take a single rnn step\n",
    "        y, h = self._rnn_step(x, input_hidden)\n",
    "\n",
    "        # Dividing the unnormalized probabilities by the temperature before \n",
    "        # tf.multinomial is equivalent to adding temperature to a softmax\n",
    "        # before sampling\n",
    "        y_temperature = y / temperature\n",
    "\n",
    "        # We use tf.squeeze to remove the unnecessary [batch, num_samples] dims\n",
    "        # We do not manually softmax - tf.multinomial softmaxes the tensor we pass it\n",
    "        next_sample = tf.squeeze(tf.multinomial(y_temperature, 1))\n",
    "\n",
    "        return next_sample, h\n",
    "\n",
    "\n",
    "    def _build_graph(self):\n",
    "        '''Build the computation graphs for training and sampling.\n",
    "\n",
    "        All placeholders to be fed in and ops / tensors to be run / output defined here.'''\n",
    "\n",
    "\n",
    "        '''Sampling graph'''\n",
    "        self.sample_input_char = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.sample_input_hidden = tf.placeholder(dtype=tf.float32, shape=[1, self.hidden_size])\n",
    "\n",
    "        self.temperature = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        self.next_sample, self.next_hidden = self._sample_one(\n",
    "            self.sample_input_char, self.sample_input_hidden, self.temperature)\n",
    "\n",
    "\n",
    "        '''Training graph'''\n",
    "        self.inputs = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.targets = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.predictions = self._forward(self.inputs)\n",
    "\n",
    "        cost_per_timestep_per_example = [\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.predictions[t],\n",
    "                    labels=self.targets[:, t])\n",
    "                for t in range(self.seq_length)\n",
    "        ]\n",
    "\n",
    "        # Use reduce_mean rather than reduce_sum over the examples in batch so that\n",
    "        # we don't need to change the learning rate when we change the batch size.\n",
    "        cost_per_timestep = [tf.reduce_mean(cost) for cost in cost_per_timestep_per_example]\n",
    "\n",
    "        # Use reduce_mean here too so we don't need to change the learning rate when\n",
    "        # we change number of timesteps.\n",
    "        self.cost = tf.reduce_mean(cost_per_timestep)\n",
    "\n",
    "        # Decay the learning rate according to a schedule.\n",
    "        self.learning_rate = tf.train.exponential_decay(self.initial_learning_rate,\n",
    "                                                        self.global_step,\n",
    "                                                        self.decay_steps,\n",
    "                                                        self.decay_factor)\n",
    "        \n",
    "        self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(\n",
    "            self.cost, global_step=self.global_step)\n",
    "\n",
    "\n",
    "        '''Finished creating graph: start session and init vars'''\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def run_train(self, input_chars, target_chars):\n",
    "        '''Call this from outside the class to run a train step'''\n",
    "        cost, lr, _ = self.sess.run([self.cost, self.learning_rate, self.train_step],\n",
    "                                   feed_dict={\n",
    "                                       self.inputs: input_chars,\n",
    "                                       self.targets: target_chars\n",
    "                                   })\n",
    "        return cost, lr\n",
    "\n",
    "\n",
    "    def run_sample(self, n, starter_character, temperature=1.0):\n",
    "        '''Call this from outside the class to sample a length-n sequence from the model'''\n",
    "\n",
    "        sampled_chars = [_ for _ in range(n)]\n",
    "        current_char = starter_character\n",
    "        h = np.zeros([1, self.hidden_size])\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            current_char, h = self.sess.run(\n",
    "                [self.next_sample, self.next_hidden],\n",
    "                feed_dict={\n",
    "                    self.sample_input_char: current_char,\n",
    "                    self.sample_input_hidden: h,\n",
    "                    self.temperature: temperature})\n",
    "\n",
    "            sampled_chars[i] = current_char\n",
    "\n",
    "        return sampled_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Train and sample from our model'''\n",
    "\n",
    "# data I/O\n",
    "data = open('shakespeare.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "embedding_size = 32 # size of embedding\n",
    "hidden_size = 256 # size of hidden layers of neurons\n",
    "seq_length = 50 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2\n",
    "decay_steps = 500\n",
    "decay_factor = 0.9\n",
    "sample_len = 500\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "n_train_steps = 25000\n",
    "\n",
    "# model parameters\n",
    "rnn = RNN(batch_size, embedding_size, hidden_size, vocab_size, \n",
    "          seq_length, learning_rate, decay_steps, decay_factor, \n",
    "          sample_len)\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "for n in range(n_train_steps):\n",
    "    \n",
    "    # prepare inputs \n",
    "    inputs = np.empty([batch_size, seq_length])\n",
    "    targets = np.empty([batch_size, seq_length])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # randomly index into the data for each example in batch\n",
    "        random_index = int(np.random.rand() * (data_size - seq_length - 1))\n",
    "        inputs[i, :] = [char_to_ix[ch] for ch in data[random_index:random_index+seq_length]]\n",
    "        targets[i, :] = [char_to_ix[ch] for ch in data[random_index+1:random_index+seq_length+1]]\n",
    "        \n",
    "    loss, lr = rnn.run_train(inputs, targets)\n",
    "    \n",
    "    # print progress\n",
    "    if n % 100 == 0: \n",
    "        print 'iter %d, loss: %f, learning rate: %f' % (n, loss, lr) \n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 1.0)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is the cost after 10,000 train steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Let's try sampling with high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 100)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Now with very low temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_ix = rnn.run_sample(sample_len, inputs[0, 0], 0.001)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How do the samples qualitatively change? What effect does temperature have, mathematically, on the output distribution?\n",
    "\n",
    "In the softmax function with a temperature T, we use e^(x_i / T) instead of e^(x_i):\n",
    "\n",
    "output_i = e^(x_i / T) / Z,\n",
    "where Z is the normalizer: Z = sum_j e^(x_j / T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
